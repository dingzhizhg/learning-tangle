{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 15:15:14.481771: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-19 15:15:15.062010: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-19 15:15:16.301724: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-19 15:15:16.301825: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-19 15:15:16.303355: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-19 15:15:16.763262: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-19 15:15:16.798087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 15:15:25.497044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # 保持numpy数组格式，只进行归一化\n",
    "    return image / 255.0\n",
    "\n",
    "def generate_data(x, y, num_clients, alpha):\n",
    "    num_classes = 10\n",
    "    # 使用字典存储每个客户端的数据，但保持numpy数组格式\n",
    "    client_data = {str(i): {'x': [], 'y': []} for i in range(num_clients)}\n",
    "    \n",
    "    # 为每个类别生成分布\n",
    "    for k in range(num_classes):\n",
    "        # 获取当前类别的所有样本\n",
    "        idx_k = [i for i, label in enumerate(y) if label == k]\n",
    "        random.shuffle(idx_k)\n",
    "        \n",
    "        # 使用Dirichlet分布生成每个客户端的样本比例\n",
    "        proportions = dirichlet.rvs(alpha * np.ones(num_clients))[0]\n",
    "        \n",
    "        # 计算每个客户端应该获得的样本数量\n",
    "        proportions = [int(p * len(idx_k)) for p in proportions]\n",
    "        \n",
    "        # 确保所有样本都被分配\n",
    "        # proportions[-1] = len(idx_k) - sum(proportions[:-1])\n",
    "        \n",
    "        # 分配样本给各个客户端\n",
    "        start_idx = 0\n",
    "        for i in range(num_clients):\n",
    "            end_idx = start_idx + proportions[i]\n",
    "            if end_idx > start_idx:  # 确保有样本可分配\n",
    "                # 保持numpy数组格式\n",
    "                client_data[str(i)]['x'].append(x[idx_k[start_idx:end_idx]])\n",
    "                client_data[str(i)]['y'].append(y[idx_k[start_idx:end_idx]])\n",
    "            start_idx = end_idx\n",
    "    \n",
    "    # 将每个客户端的数据转换为numpy数组\n",
    "    for client_id in client_data:\n",
    "        if client_data[client_id]['x']:  # 确保有数据\n",
    "            client_data[client_id]['x'] = np.concatenate(client_data[client_id]['x'], axis=0)\n",
    "            client_data[client_id]['y'] = np.concatenate(client_data[client_id]['y'], axis=0)\n",
    "    \n",
    "    return client_data\n",
    "\n",
    "def get_cluster_id(labels):\n",
    "    counts = np.bincount(labels, minlength=10)\n",
    "    return int(np.argmax(counts))\n",
    "\n",
    "# 修改输出格式，确保numpy数组被正确序列化\n",
    "def numpy_to_list(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [numpy_to_list(item) for item in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "# 使用全部数据\n",
    "train_size = int(len(x_train) * 1.0)\n",
    "test_size = int(len(x_test) * 1.0)\n",
    "\n",
    "# train_size = int(len(x_train) * 0.2)\n",
    "# test_size = int(len(x_test) * 0.2)\n",
    "\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择数据\n",
    "train_indices = np.random.choice(len(x_train), train_size, replace=False)\n",
    "test_indices = np.random.choice(len(x_test), test_size, replace=False)\n",
    "\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "x_test = x_test[test_indices]\n",
    "y_test = y_test[test_indices]\n",
    "\n",
    "# 数据预处理并保持numpy数组格式\n",
    "x_train = np.array([preprocess_image(img) for img in x_train])\n",
    "x_test = np.array([preprocess_image(img) for img in x_test])\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 生成训练和测试数据\n",
    "num_clients = 100\n",
    "alpha = 100  # Dirichlet分布的参数，越小越non-IID\n",
    "\n",
    "train_data = generate_data(x_train, y_train, num_clients, alpha)\n",
    "test_data = generate_data(x_test, y_test, num_clients, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个客户端生成cluster_id\n",
    "cluster_ids = {client_id: get_cluster_id(data['y']) \n",
    "                for client_id, data in train_data.items()}\n",
    "\n",
    "# 保存数据前转换numpy数组为列表\n",
    "train_output = {\n",
    "    'user_data': numpy_to_list(train_data),\n",
    "    'cluster_ids': list(cluster_ids.values()),\n",
    "    'users': list(train_data.keys())\n",
    "}\n",
    "\n",
    "test_output = {\n",
    "    'user_data': numpy_to_list(test_data),\n",
    "    'cluster_ids': list(cluster_ids.values()),\n",
    "    'users': list(train_data.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 100\n",
      "Average training samples per client: 495.02\n",
      "Average test samples per client: 95.06\n",
      "Cluster distribution: [12 11  9 11  8 11 13 10  7  8]\n",
      "\n",
      "每个客户端的数据量:\n",
      "客户端 0: 训练集 480 样本, 测试集 92 样本\n",
      "客户端 1: 训练集 492 样本, 测试集 92 样本\n",
      "客户端 10: 训练集 535 样本, 测试集 89 样本\n",
      "客户端 11: 训练集 496 样本, 测试集 91 样本\n",
      "客户端 12: 训练集 472 样本, 测试集 97 样本\n",
      "客户端 13: 训练集 462 样本, 测试集 91 样本\n",
      "客户端 14: 训练集 476 样本, 测试集 96 样本\n",
      "客户端 15: 训练集 494 样本, 测试集 94 样本\n",
      "客户端 16: 训练集 482 样本, 测试集 93 样本\n",
      "客户端 17: 训练集 485 样本, 测试集 102 样本\n",
      "客户端 18: 训练集 499 样本, 测试集 92 样本\n",
      "客户端 19: 训练集 484 样本, 测试集 98 样本\n",
      "客户端 2: 训练集 485 样本, 测试集 96 样本\n",
      "客户端 20: 训练集 488 样本, 测试集 95 样本\n",
      "客户端 21: 训练集 509 样本, 测试集 94 样本\n",
      "客户端 22: 训练集 518 样本, 测试集 95 样本\n",
      "客户端 23: 训练集 512 样本, 测试集 97 样本\n",
      "客户端 24: 训练集 467 样本, 测试集 93 样本\n",
      "客户端 25: 训练集 491 样本, 测试集 94 样本\n",
      "客户端 26: 训练集 473 样本, 测试集 99 样本\n",
      "客户端 27: 训练集 499 样本, 测试集 94 样本\n",
      "客户端 28: 训练集 465 样本, 测试集 95 样本\n",
      "客户端 29: 训练集 516 样本, 测试集 95 样本\n",
      "客户端 3: 训练集 488 样本, 测试集 98 样本\n",
      "客户端 30: 训练集 513 样本, 测试集 91 样本\n",
      "客户端 31: 训练集 515 样本, 测试集 91 样本\n",
      "客户端 32: 训练集 485 样本, 测试集 88 样本\n",
      "客户端 33: 训练集 494 样本, 测试集 100 样本\n",
      "客户端 34: 训练集 509 样本, 测试集 89 样本\n",
      "客户端 35: 训练集 487 样本, 测试集 97 样本\n",
      "客户端 36: 训练集 498 样本, 测试集 92 样本\n",
      "客户端 37: 训练集 509 样本, 测试集 99 样本\n",
      "客户端 38: 训练集 501 样本, 测试集 92 样本\n",
      "客户端 39: 训练集 489 样本, 测试集 86 样本\n",
      "客户端 4: 训练集 507 样本, 测试集 95 样本\n",
      "客户端 40: 训练集 514 样本, 测试集 95 样本\n",
      "客户端 41: 训练集 491 样本, 测试集 95 样本\n",
      "客户端 42: 训练集 496 样本, 测试集 94 样本\n",
      "客户端 43: 训练集 499 样本, 测试集 93 样本\n",
      "客户端 44: 训练集 467 样本, 测试集 90 样本\n",
      "客户端 45: 训练集 528 样本, 测试集 89 样本\n",
      "客户端 46: 训练集 496 样本, 测试集 92 样本\n",
      "客户端 47: 训练集 534 样本, 测试集 93 样本\n",
      "客户端 48: 训练集 520 样本, 测试集 94 样本\n",
      "客户端 49: 训练集 480 样本, 测试集 94 样本\n",
      "客户端 5: 训练集 524 样本, 测试集 97 样本\n",
      "客户端 50: 训练集 497 样本, 测试集 101 样本\n",
      "客户端 51: 训练集 468 样本, 测试集 97 样本\n",
      "客户端 52: 训练集 509 样本, 测试集 99 样本\n",
      "客户端 53: 训练集 507 样本, 测试集 100 样本\n",
      "客户端 54: 训练集 503 样本, 测试集 89 样本\n",
      "客户端 55: 训练集 514 样本, 测试集 89 样本\n",
      "客户端 56: 训练集 489 样本, 测试集 93 样本\n",
      "客户端 57: 训练集 493 样本, 测试集 93 样本\n",
      "客户端 58: 训练集 497 样本, 测试集 93 样本\n",
      "客户端 59: 训练集 484 样本, 测试集 95 样本\n",
      "客户端 6: 训练集 487 样本, 测试集 96 样本\n",
      "客户端 60: 训练集 483 样本, 测试集 99 样本\n",
      "客户端 61: 训练集 529 样本, 测试集 98 样本\n",
      "客户端 62: 训练集 509 样本, 测试集 90 样本\n",
      "客户端 63: 训练集 471 样本, 测试集 100 样本\n",
      "客户端 64: 训练集 502 样本, 测试集 91 样本\n",
      "客户端 65: 训练集 504 样本, 测试集 97 样本\n",
      "客户端 66: 训练集 491 样本, 测试集 94 样本\n",
      "客户端 67: 训练集 503 样本, 测试集 87 样本\n",
      "客户端 68: 训练集 482 样本, 测试集 98 样本\n",
      "客户端 69: 训练集 488 样本, 测试集 97 样本\n",
      "客户端 7: 训练集 485 样本, 测试集 95 样本\n",
      "客户端 70: 训练集 484 样本, 测试集 100 样本\n",
      "客户端 71: 训练集 485 样本, 测试集 96 样本\n",
      "客户端 72: 训练集 489 样本, 测试集 101 样本\n",
      "客户端 73: 训练集 488 样本, 测试集 96 样本\n",
      "客户端 74: 训练集 502 样本, 测试集 101 样本\n",
      "客户端 75: 训练集 467 样本, 测试集 105 样本\n",
      "客户端 76: 训练集 500 样本, 测试集 97 样本\n",
      "客户端 77: 训练集 507 样本, 测试集 95 样本\n",
      "客户端 78: 训练集 495 样本, 测试集 94 样本\n",
      "客户端 79: 训练集 499 样本, 测试集 97 样本\n",
      "客户端 8: 训练集 505 样本, 测试集 92 样本\n",
      "客户端 80: 训练集 487 样本, 测试集 94 样本\n",
      "客户端 81: 训练集 490 样本, 测试集 95 样本\n",
      "客户端 82: 训练集 491 样本, 测试集 89 样本\n",
      "客户端 83: 训练集 517 样本, 测试集 98 样本\n",
      "客户端 84: 训练集 484 样本, 测试集 100 样本\n",
      "客户端 85: 训练集 479 样本, 测试集 96 样本\n",
      "客户端 86: 训练集 513 样本, 测试集 96 样本\n",
      "客户端 87: 训练集 510 样本, 测试集 96 样本\n",
      "客户端 88: 训练集 466 样本, 测试集 94 样本\n",
      "客户端 89: 训练集 511 样本, 测试集 94 样本\n",
      "客户端 9: 训练集 495 样本, 测试集 97 样本\n",
      "客户端 90: 训练集 502 样本, 测试集 99 样本\n",
      "客户端 91: 训练集 506 样本, 测试集 93 样本\n",
      "客户端 92: 训练集 476 样本, 测试集 91 样本\n",
      "客户端 93: 训练集 493 样本, 测试集 101 样本\n",
      "客户端 94: 训练集 486 样本, 测试集 99 样本\n",
      "客户端 95: 训练集 482 样本, 测试集 104 样本\n",
      "客户端 96: 训练集 517 样本, 测试集 100 样本\n",
      "客户端 97: 训练集 493 样本, 测试集 93 样本\n",
      "客户端 98: 训练集 485 样本, 测试集 97 样本\n",
      "客户端 99: 训练集 479 样本, 测试集 97 样本\n"
     ]
    }
   ],
   "source": [
    "# 打印统计信息\n",
    "print(f\"Number of clients: {num_clients}\")\n",
    "print(f\"Average training samples per client: {np.mean([len(data['x']) for data in train_data.values()])}\")\n",
    "print(f\"Average test samples per client: {np.mean([len(data['x']) for data in test_data.values()])}\")\n",
    "print(f\"Cluster distribution: {np.bincount(list(cluster_ids.values()), minlength=10)}\")\n",
    "    \n",
    "# 打印每个客户端的数据量\n",
    "print(\"\\n每个客户端的数据量:\")\n",
    "for client_id in sorted(train_data.keys()):\n",
    "    train_samples = len(train_data[client_id]['x'])\n",
    "    test_samples = len(test_data[client_id]['x'])\n",
    "    print(f\"客户端 {client_id}: 训练集 {train_samples} 样本, 测试集 {test_samples} 样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出目录\n",
    "os.makedirs('/root/learning-tangle/leaf/data/cifar10/data/train', exist_ok=True)\n",
    "os.makedirs('/root/learning-tangle/leaf/data/cifar10/data/test', exist_ok=True)\n",
    "\n",
    "# 保存数据\n",
    "with open('/root/learning-tangle/leaf/data/cifar10/data/train/data.json', 'w') as file:\n",
    "    json.dump(train_output, file)\n",
    "with open('/root/learning-tangle/leaf/data/cifar10/data/test/data.json', 'w') as file:\n",
    "    json.dump(test_output, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
