{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 01:06:03.860982: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-19 01:06:03.882832: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-19 01:06:04.638363: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-19 01:06:04.931594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755565565.052213  124617 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755565565.090331  124617 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755565565.551417  124617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755565565.551481  124617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755565565.551487  124617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755565565.551492  124617 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-19 01:06:05.608293: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # 保持numpy数组格式，只进行归一化\n",
    "    return image / 255.0\n",
    "\n",
    "def generate_data(x, y, num_clients, alpha):\n",
    "    num_classes = 10\n",
    "    client_data = {str(i): {'x': [], 'y': []} for i in range(num_clients)}\n",
    "    \n",
    "    for k in range(num_classes):\n",
    "        idx_k = [i for i, label in enumerate(y) if label == k]\n",
    "        random.shuffle(idx_k)\n",
    "        n_k = len(idx_k)\n",
    "        if n_k == 0:\n",
    "            continue\n",
    "            \n",
    "        # 使用更小的alpha值，让分布更均匀\n",
    "        proportions = dirichlet.rvs(alpha * np.ones(num_clients))[0]\n",
    "        proportions = proportions / proportions.sum()  # 归一化\n",
    "        \n",
    "        # 按比例分配样本数\n",
    "        proportions = (proportions * n_k).astype(int)\n",
    "        \n",
    "        # 改进的修正算法：随机分配剩余样本\n",
    "        remaining = n_k - proportions.sum()\n",
    "        if remaining > 0:\n",
    "            # 随机选择客户端分配剩余样本\n",
    "            indices = np.random.choice(num_clients, remaining, replace=False)\n",
    "            for idx in indices:\n",
    "                proportions[idx] += 1\n",
    "        elif remaining < 0:\n",
    "            # 随机减少样本\n",
    "            indices = np.random.choice(num_clients, abs(remaining), replace=False)\n",
    "            for idx in indices:\n",
    "                if proportions[idx] > 0:\n",
    "                    proportions[idx] -= 1\n",
    "        \n",
    "        # 分配样本\n",
    "        start = 0\n",
    "        for i, count in enumerate(proportions):\n",
    "            if count > 0:\n",
    "                client_data[str(i)]['x'].append(x[idx_k[start:start+count]])\n",
    "                client_data[str(i)]['y'].append(y[idx_k[start:start+count]])\n",
    "            start += count\n",
    "    \n",
    "    # 合并每个客户端的数据\n",
    "    for client_id in client_data:\n",
    "        if client_data[client_id]['x']:\n",
    "            client_data[client_id]['x'] = np.concatenate(client_data[client_id]['x'], axis=0)\n",
    "            client_data[client_id]['y'] = np.concatenate(client_data[client_id]['y'], axis=0)\n",
    "        else:\n",
    "            client_data[client_id]['x'] = np.array([])\n",
    "            client_data[client_id]['y'] = np.array([])\n",
    "    \n",
    "    # 保证每个客户端至少有1个样本\n",
    "    empty_clients = [cid for cid, data in client_data.items() if len(data['x']) == 0]\n",
    "    for cid in empty_clients:\n",
    "        # 从样本最多的客户端借一个\n",
    "        max_cid = max(client_data, key=lambda k: len(client_data[k]['x']))\n",
    "        client_data[cid]['x'] = client_data[max_cid]['x'][:1]\n",
    "        client_data[cid]['y'] = client_data[max_cid]['y'][:1]\n",
    "        client_data[max_cid]['x'] = client_data[max_cid]['x'][1:]\n",
    "        client_data[max_cid]['y'] = client_data[max_cid]['y'][1:]\n",
    "    \n",
    "    return client_data\n",
    "\n",
    "def get_cluster_id(labels):\n",
    "    counts = np.bincount(labels, minlength=10)\n",
    "    return int(np.argmax(counts))\n",
    "\n",
    "# 修改输出格式，确保numpy数组被正确序列化\n",
    "def numpy_to_list(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: numpy_to_list(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [numpy_to_list(item) for item in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9us/step \n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "# 使用全部数据\n",
    "train_size = int(len(x_train) * 1.0)\n",
    "test_size = int(len(x_test) * 1.0)\n",
    "\n",
    "# train_size = int(len(x_train) * 0.2)\n",
    "# test_size = int(len(x_test) * 0.2)\n",
    "\n",
    "print(train_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择数据\n",
    "train_indices = np.random.choice(len(x_train), train_size, replace=False)\n",
    "test_indices = np.random.choice(len(x_test), test_size, replace=False)\n",
    "\n",
    "x_train = x_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "x_test = x_test[test_indices]\n",
    "y_test = y_test[test_indices]\n",
    "\n",
    "# 数据预处理并保持numpy数组格式\n",
    "x_train = np.array([preprocess_image(img) for img in x_train])\n",
    "x_test = np.array([preprocess_image(img) for img in x_test])\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# 生成训练和测试数据\n",
    "num_clients = 100\n",
    "alpha = 1000  # Dirichlet分布的参数，越小越non-IID\n",
    "\n",
    "train_data = generate_data(x_train, y_train, num_clients, alpha)\n",
    "test_data = generate_data(x_test, y_test, num_clients, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为每个客户端生成cluster_id\n",
    "cluster_ids = {client_id: get_cluster_id(data['y']) \n",
    "                for client_id, data in train_data.items()}\n",
    "\n",
    "# 保存数据前转换numpy数组为列表\n",
    "train_output = {\n",
    "    'user_data': numpy_to_list(train_data),\n",
    "    'cluster_ids': list(cluster_ids.values()),\n",
    "    'users': list(train_data.keys())\n",
    "}\n",
    "\n",
    "test_output = {\n",
    "    'user_data': numpy_to_list(test_data),\n",
    "    'cluster_ids': list(cluster_ids.values()),\n",
    "    'users': list(train_data.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clients: 100\n",
      "Average training samples per client: 600.0\n",
      "Average test samples per client: 100.0\n",
      "Cluster distribution: [18 16 12  9  5 10  7 10  5  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0]\n",
      "\n",
      "每个客户端的数据量:\n",
      "客户端 0: 训练集 603 样本, 测试集 102 样本\n",
      "客户端 1: 训练集 595 样本, 测试集 98 样本\n",
      "客户端 10: 训练集 584 样本, 测试集 102 样本\n",
      "客户端 11: 训练集 597 样本, 测试集 101 样本\n",
      "客户端 12: 训练集 601 样本, 测试集 103 样本\n",
      "客户端 13: 训练集 601 样本, 测试集 98 样本\n",
      "客户端 14: 训练集 597 样本, 测试集 100 样本\n",
      "客户端 15: 训练集 594 样本, 测试集 102 样本\n",
      "客户端 16: 训练集 602 样本, 测试集 98 样本\n",
      "客户端 17: 训练集 604 样本, 测试集 99 样本\n",
      "客户端 18: 训练集 585 样本, 测试集 99 样本\n",
      "客户端 19: 训练集 607 样本, 测试集 98 样本\n",
      "客户端 2: 训练集 615 样本, 测试集 99 样本\n",
      "客户端 20: 训练集 594 样本, 测试集 100 样本\n",
      "客户端 21: 训练集 600 样本, 测试集 101 样本\n",
      "客户端 22: 训练集 604 样本, 测试集 101 样本\n",
      "客户端 23: 训练集 590 样本, 测试集 100 样本\n",
      "客户端 24: 训练集 597 样本, 测试集 97 样本\n",
      "客户端 25: 训练集 590 样本, 测试集 101 样本\n",
      "客户端 26: 训练集 604 样本, 测试集 100 样本\n",
      "客户端 27: 训练集 602 样本, 测试集 102 样本\n",
      "客户端 28: 训练集 609 样本, 测试集 102 样本\n",
      "客户端 29: 训练集 601 样本, 测试集 101 样本\n",
      "客户端 3: 训练集 601 样本, 测试集 103 样本\n",
      "客户端 30: 训练集 590 样本, 测试集 99 样本\n",
      "客户端 31: 训练集 597 样本, 测试集 99 样本\n",
      "客户端 32: 训练集 586 样本, 测试集 99 样本\n",
      "客户端 33: 训练集 609 样本, 测试集 97 样本\n",
      "客户端 34: 训练集 604 样本, 测试集 103 样本\n",
      "客户端 35: 训练集 597 样本, 测试集 103 样本\n",
      "客户端 36: 训练集 605 样本, 测试集 102 样本\n",
      "客户端 37: 训练集 599 样本, 测试集 97 样本\n",
      "客户端 38: 训练集 594 样本, 测试集 98 样本\n",
      "客户端 39: 训练集 595 样本, 测试集 98 样本\n",
      "客户端 4: 训练集 590 样本, 测试集 99 样本\n",
      "客户端 40: 训练集 590 样本, 测试集 100 样本\n",
      "客户端 41: 训练集 602 样本, 测试集 99 样本\n",
      "客户端 42: 训练集 599 样本, 测试集 96 样本\n",
      "客户端 43: 训练集 587 样本, 测试集 100 样本\n",
      "客户端 44: 训练集 602 样本, 测试集 96 样本\n",
      "客户端 45: 训练集 597 样本, 测试集 101 样本\n",
      "客户端 46: 训练集 594 样本, 测试集 94 样本\n",
      "客户端 47: 训练集 600 样本, 测试集 101 样本\n",
      "客户端 48: 训练集 606 样本, 测试集 97 样本\n",
      "客户端 49: 训练集 601 样本, 测试集 99 样本\n",
      "客户端 5: 训练集 604 样本, 测试集 96 样本\n",
      "客户端 50: 训练集 600 样本, 测试集 100 样本\n",
      "客户端 51: 训练集 607 样本, 测试集 97 样本\n",
      "客户端 52: 训练集 597 样本, 测试集 102 样本\n",
      "客户端 53: 训练集 609 样本, 测试集 99 样本\n",
      "客户端 54: 训练集 593 样本, 测试集 101 样本\n",
      "客户端 55: 训练集 604 样本, 测试集 102 样本\n",
      "客户端 56: 训练集 594 样本, 测试集 99 样本\n",
      "客户端 57: 训练集 606 样本, 测试集 99 样本\n",
      "客户端 58: 训练集 598 样本, 测试集 101 样本\n",
      "客户端 59: 训练集 602 样本, 测试集 104 样本\n",
      "客户端 6: 训练集 613 样本, 测试集 100 样本\n",
      "客户端 60: 训练集 600 样本, 测试集 104 样本\n",
      "客户端 61: 训练集 600 样本, 测试集 99 样本\n",
      "客户端 62: 训练集 602 样本, 测试集 104 样本\n",
      "客户端 63: 训练集 609 样本, 测试集 103 样本\n",
      "客户端 64: 训练集 604 样本, 测试集 99 样本\n",
      "客户端 65: 训练集 595 样本, 测试集 107 样本\n",
      "客户端 66: 训练集 585 样本, 测试集 102 样本\n",
      "客户端 67: 训练集 604 样本, 测试集 102 样本\n",
      "客户端 68: 训练集 608 样本, 测试集 100 样本\n",
      "客户端 69: 训练集 599 样本, 测试集 94 样本\n",
      "客户端 7: 训练集 593 样本, 测试集 100 样本\n",
      "客户端 70: 训练集 591 样本, 测试集 103 样本\n",
      "客户端 71: 训练集 602 样本, 测试集 100 样本\n",
      "客户端 72: 训练集 596 样本, 测试集 101 样本\n",
      "客户端 73: 训练集 604 样本, 测试集 101 样本\n",
      "客户端 74: 训练集 599 样本, 测试集 98 样本\n",
      "客户端 75: 训练集 599 样本, 测试集 106 样本\n",
      "客户端 76: 训练集 599 样本, 测试集 103 样本\n",
      "客户端 77: 训练集 603 样本, 测试集 100 样本\n",
      "客户端 78: 训练集 595 样本, 测试集 98 样本\n",
      "客户端 79: 训练集 603 样本, 测试集 102 样本\n",
      "客户端 8: 训练集 603 样本, 测试集 100 样本\n",
      "客户端 80: 训练集 605 样本, 测试集 97 样本\n",
      "客户端 81: 训练集 612 样本, 测试集 100 样本\n",
      "客户端 82: 训练集 602 样本, 测试集 99 样本\n",
      "客户端 83: 训练集 609 样本, 测试集 104 样本\n",
      "客户端 84: 训练集 606 样本, 测试集 100 样本\n",
      "客户端 85: 训练集 603 样本, 测试集 98 样本\n",
      "客户端 86: 训练集 603 样本, 测试集 99 样本\n",
      "客户端 87: 训练集 608 样本, 测试集 98 样本\n",
      "客户端 88: 训练集 604 样本, 测试集 99 样本\n",
      "客户端 89: 训练集 601 样本, 测试集 100 样本\n",
      "客户端 9: 训练集 600 样本, 测试集 99 样本\n",
      "客户端 90: 训练集 602 样本, 测试集 102 样本\n",
      "客户端 91: 训练集 603 样本, 测试集 101 样本\n",
      "客户端 92: 训练集 603 样本, 测试集 95 样本\n",
      "客户端 93: 训练集 592 样本, 测试集 101 样本\n",
      "客户端 94: 训练集 597 样本, 测试集 97 样本\n",
      "客户端 95: 训练集 593 样本, 测试集 96 样本\n",
      "客户端 96: 训练集 608 样本, 测试集 103 样本\n",
      "客户端 97: 训练集 597 样本, 测试集 101 样本\n",
      "客户端 98: 训练集 607 样本, 测试集 99 样本\n",
      "客户端 99: 训练集 604 样本, 测试集 102 样本\n"
     ]
    }
   ],
   "source": [
    "# 打印统计信息\n",
    "print(f\"Number of clients: {num_clients}\")\n",
    "print(f\"Average training samples per client: {np.mean([len(data['x']) for data in train_data.values()])}\")\n",
    "print(f\"Average test samples per client: {np.mean([len(data['x']) for data in test_data.values()])}\")\n",
    "print(f\"Cluster distribution: {np.bincount(list(cluster_ids.values()), minlength=100)}\") \n",
    "\n",
    "# 打印每个客户端的数据量\n",
    "print(\"\\n每个客户端的数据量:\")\n",
    "for client_id in sorted(train_data.keys()):\n",
    "    train_samples = len(train_data[client_id]['x'])\n",
    "    test_samples = len(test_data[client_id]['x'])\n",
    "    print(f\"客户端 {client_id}: 训练集 {train_samples} 样本, 测试集 {test_samples} 样本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建输出目录\n",
    "os.makedirs('/root/learning-tangle/leaf/data/fmnist/data/train', exist_ok=True)\n",
    "os.makedirs('/root/learning-tangle/leaf/data/fmnist/data/test', exist_ok=True)\n",
    "\n",
    "# 保存数据\n",
    "with open('/root/learning-tangle/leaf/data/fmnist/data/train/data.json', 'w') as file:\n",
    "    json.dump(train_output, file)\n",
    "with open('/root/learning-tangle/leaf/data/fmnist/data/test/data.json', 'w') as file:\n",
    "    json.dump(test_output, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
